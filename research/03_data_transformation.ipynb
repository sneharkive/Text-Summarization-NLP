{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:39:36.501235Z",
     "start_time": "2026-01-29T07:39:36.481158Z"
    }
   },
   "source": [
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:39:36.529446Z",
     "start_time": "2026-01-29T07:39:36.512656Z"
    }
   },
   "source": [
    "%pwd"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SNEHA\\\\Desktop\\\\AIML_Projects\\\\Text-Summarization-NLP\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:39:36.545039Z",
     "start_time": "2026-01-29T07:39:36.532197Z"
    }
   },
   "source": [
    "os.chdir(\"../\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:39:36.571473Z",
     "start_time": "2026-01-29T07:39:36.562532Z"
    }
   },
   "source": [
    "%pwd"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SNEHA\\\\Desktop\\\\AIML_Projects\\\\Text-Summarization-NLP'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:39:36.602962Z",
     "start_time": "2026-01-29T07:39:36.595732Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    tokenizer_name: Path\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:39:36.668227Z",
     "start_time": "2026-01-29T07:39:36.609791Z"
    }
   },
   "source": [
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:39:36.685653Z",
     "start_time": "2026-01-29T07:39:36.677278Z"
    }
   },
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            tokenizer_name = config.tokenizer_name\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:39:45.674376Z",
     "start_time": "2026-01-29T07:39:36.698203Z"
    }
   },
   "source": [
    "import os\n",
    "from textSummarizer.logging import logger\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SNEHA\\Desktop\\AIML_Projects\\Text-Summarization-NLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:39:45.692847Z",
     "start_time": "2026-01-29T07:39:45.683650Z"
    }
   },
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "\n",
    "    \n",
    "    def convert_examples_to_features(self,example_batch):\n",
    "        input_encodings = self.tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            target_encodings = self.tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n",
    "            \n",
    "        return {\n",
    "            'input_ids' : input_encodings['input_ids'],\n",
    "            'attention_mask': input_encodings['attention_mask'],\n",
    "            'labels': target_encodings['input_ids']\n",
    "        }\n",
    "    \n",
    "\n",
    "    def convert(self):\n",
    "        dataset_samsum = load_from_disk(self.config.data_path)\n",
    "        dataset_samsum_pt = dataset_samsum.map(self.convert_examples_to_features, batched = True)\n",
    "        dataset_samsum_pt.save_to_disk(os.path.join(self.config.root_dir,\"samsum_dataset\"))"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:39:51.752693Z",
     "start_time": "2026-01-29T07:39:45.705915Z"
    }
   },
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.convert()\n",
    "except Exception as e:\n",
    "    raise e"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-29 13:09:45,743: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2026-01-29 13:09:45,743: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2026-01-29 13:09:45,751: INFO: common: created directory at: artifacts]\n",
      "[2026-01-29 13:09:45,752: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2026-01-29 13:09:46,887: INFO: _client: HTTP Request: HEAD https://huggingface.co/google/pegasus-cnn_dailymail/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"]\n",
      "[2026-01-29 13:09:46,987: INFO: _client: HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/google/pegasus-cnn_dailymail/40d588fdab0cc077b80d950b300bf66ad3c75b92/config.json \"HTTP/1.1 200 OK\"]\n",
      "[2026-01-29 13:09:47,373: INFO: _client: HTTP Request: HEAD https://huggingface.co/google/pegasus-cnn_dailymail/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"]\n",
      "[2026-01-29 13:09:47,575: INFO: _client: HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/google/pegasus-cnn_dailymail/40d588fdab0cc077b80d950b300bf66ad3c75b92/tokenizer_config.json \"HTTP/1.1 200 OK\"]\n",
      "[2026-01-29 13:09:48,038: INFO: _client: HTTP Request: GET https://huggingface.co/api/models/google/pegasus-cnn_dailymail/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"]\n",
      "[2026-01-29 13:09:48,480: INFO: _client: HTTP Request: GET https://huggingface.co/api/models/google/pegasus-cnn_dailymail/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "PegasusTokenizer has no attribute as_target_tokenizer",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      5\u001B[39m     data_transformation.convert()\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m     data_transformation_config = config.get_data_transformation_config()\n\u001B[32m      4\u001B[39m     data_transformation = DataTransformation(config=data_transformation_config)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m     \u001B[43mdata_transformation\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m      7\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 23\u001B[39m, in \u001B[36mDataTransformation.convert\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mconvert\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m     22\u001B[39m     dataset_samsum = load_from_disk(\u001B[38;5;28mself\u001B[39m.config.data_path)\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m     dataset_samsum_pt = \u001B[43mdataset_samsum\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconvert_examples_to_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     24\u001B[39m     dataset_samsum_pt.save_to_disk(os.path.join(\u001B[38;5;28mself\u001B[39m.config.root_dir,\u001B[33m\"\u001B[39m\u001B[33msamsum_dataset\u001B[39m\u001B[33m\"\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\AIML_Projects\\Text-Summarization-NLP\\.venv\\Lib\\site-packages\\datasets\\dataset_dict.py:953\u001B[39m, in \u001B[36mDatasetDict.map\u001B[39m\u001B[34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001B[39m\n\u001B[32m    950\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m with_split:\n\u001B[32m    951\u001B[39m     function = bind(function, split)\n\u001B[32m--> \u001B[39m\u001B[32m953\u001B[39m dataset_dict[split] = \u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    954\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    955\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwith_indices\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    956\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwith_rank\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_rank\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    957\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_columns\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    958\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatched\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatched\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    959\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    960\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdrop_last_batch\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdrop_last_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    961\u001B[39m \u001B[43m    \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    962\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkeep_in_memory\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeep_in_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    963\u001B[39m \u001B[43m    \u001B[49m\u001B[43mload_from_cache_file\u001B[49m\u001B[43m=\u001B[49m\u001B[43mload_from_cache_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    964\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_file_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_file_names\u001B[49m\u001B[43m[\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    965\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwriter_batch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwriter_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    966\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    967\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdisable_nullable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdisable_nullable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    968\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    969\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    970\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtry_original_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtry_original_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    972\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    974\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m with_split:\n\u001B[32m    975\u001B[39m     function = function.func\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\AIML_Projects\\Text-Summarization-NLP\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:562\u001B[39m, in \u001B[36mtransmit_format.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    555\u001B[39m self_format = {\n\u001B[32m    556\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._format_type,\n\u001B[32m    557\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mformat_kwargs\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._format_kwargs,\n\u001B[32m    558\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcolumns\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._format_columns,\n\u001B[32m    559\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33moutput_all_columns\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._output_all_columns,\n\u001B[32m    560\u001B[39m }\n\u001B[32m    561\u001B[39m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m562\u001B[39m out: Union[\u001B[33m\"\u001B[39m\u001B[33mDataset\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mDatasetDict\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    563\u001B[39m datasets: \u001B[38;5;28mlist\u001B[39m[\u001B[33m\"\u001B[39m\u001B[33mDataset\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mlist\u001B[39m(out.values()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[32m    564\u001B[39m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\AIML_Projects\\Text-Summarization-NLP\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3343\u001B[39m, in \u001B[36mDataset.map\u001B[39m\u001B[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001B[39m\n\u001B[32m   3341\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3342\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m unprocessed_kwargs \u001B[38;5;129;01min\u001B[39;00m unprocessed_kwargs_per_job:\n\u001B[32m-> \u001B[39m\u001B[32m3343\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mDataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_map_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43munprocessed_kwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   3344\u001B[39m \u001B[43m                \u001B[49m\u001B[43mcheck_if_shard_done\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3346\u001B[39m \u001B[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\AIML_Projects\\Text-Summarization-NLP\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3699\u001B[39m, in \u001B[36mDataset._map_single\u001B[39m\u001B[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001B[39m\n\u001B[32m   3697\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3698\u001B[39m     _time = time.time()\n\u001B[32m-> \u001B[39m\u001B[32m3699\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miter_outputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshard_iterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   3700\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_examples_in_batch\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3701\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mupdate_data\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\AIML_Projects\\Text-Summarization-NLP\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3649\u001B[39m, in \u001B[36mDataset._map_single.<locals>.iter_outputs\u001B[39m\u001B[34m(shard_iterable)\u001B[39m\n\u001B[32m   3647\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3648\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i, example \u001B[38;5;129;01min\u001B[39;00m shard_iterable:\n\u001B[32m-> \u001B[39m\u001B[32m3649\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m i, \u001B[43mapply_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[43m=\u001B[49m\u001B[43moffset\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\AIML_Projects\\Text-Summarization-NLP\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3572\u001B[39m, in \u001B[36mDataset._map_single.<locals>.apply_function\u001B[39m\u001B[34m(pa_inputs, indices, offset)\u001B[39m\n\u001B[32m   3570\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001B[39;00m\n\u001B[32m   3571\u001B[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001B[32m-> \u001B[39m\u001B[32m3572\u001B[39m processed_inputs = \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfn_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43madditional_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3573\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 11\u001B[39m, in \u001B[36mDataTransformation.convert_examples_to_features\u001B[39m\u001B[34m(self, example_batch)\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mconvert_examples_to_features\u001B[39m(\u001B[38;5;28mself\u001B[39m,example_batch):\n\u001B[32m      9\u001B[39m     input_encodings = \u001B[38;5;28mself\u001B[39m.tokenizer(example_batch[\u001B[33m'\u001B[39m\u001B[33mdialogue\u001B[39m\u001B[33m'\u001B[39m] , max_length = \u001B[32m1024\u001B[39m, truncation = \u001B[38;5;28;01mTrue\u001B[39;00m )\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mas_target_tokenizer\u001B[49m():\n\u001B[32m     12\u001B[39m         target_encodings = \u001B[38;5;28mself\u001B[39m.tokenizer(example_batch[\u001B[33m'\u001B[39m\u001B[33msummary\u001B[39m\u001B[33m'\u001B[39m], max_length = \u001B[32m128\u001B[39m, truncation = \u001B[38;5;28;01mTrue\u001B[39;00m )\n\u001B[32m     14\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[32m     15\u001B[39m         \u001B[33m'\u001B[39m\u001B[33minput_ids\u001B[39m\u001B[33m'\u001B[39m : input_encodings[\u001B[33m'\u001B[39m\u001B[33minput_ids\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m     16\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mattention_mask\u001B[39m\u001B[33m'\u001B[39m: input_encodings[\u001B[33m'\u001B[39m\u001B[33mattention_mask\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m     17\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mlabels\u001B[39m\u001B[33m'\u001B[39m: target_encodings[\u001B[33m'\u001B[39m\u001B[33minput_ids\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m     18\u001B[39m     }\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\AIML_Projects\\Text-Summarization-NLP\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1326\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.__getattr__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   1323\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.convert_tokens_to_ids([\u001B[38;5;28mstr\u001B[39m(tok) \u001B[38;5;28;01mfor\u001B[39;00m tok \u001B[38;5;129;01min\u001B[39;00m tokens]) \u001B[38;5;28;01mif\u001B[39;00m tokens \u001B[38;5;28;01melse\u001B[39;00m []\n\u001B[32m   1325\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.\u001B[34m__dict__\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1326\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m has no attribute \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   1327\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1328\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__getattr__\u001B[39m(key)\n",
      "\u001B[31mAttributeError\u001B[39m: PegasusTokenizer has no attribute as_target_tokenizer"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T07:39:51.752693200Z",
     "start_time": "2026-01-29T07:02:38.257799Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
