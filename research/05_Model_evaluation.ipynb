{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import os\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"'d:\\\\\\\\Bappy\\\\\\\\YouTube\\\\\\\\Text-Summarizer-Project\\\\\\\\research'\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 2,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"%pwd\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"os.chdir(\\\"../\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 4,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"'d:\\\\\\\\Bappy\\\\\\\\YouTube\\\\\\\\Text-Summarizer-Project'\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 4,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"%pwd\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from dataclasses import dataclass\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"@dataclass(frozen=True)\\n\",\n",
    "    \"class ModelEvaluationConfig:\\n\",\n",
    "    \"    root_dir: Path\\n\",\n",
    "    \"    data_path: Path\\n\",\n",
    "    \"    model_path: Path\\n\",\n",
    "    \"    tokenizer_path: Path\\n\",\n",
    "    \"    metric_file_name: Path\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from textSummarizer.constants import *\\n\",\n",
    "    \"from textSummarizer.utils.common import read_yaml, create_directories\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 7,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class ConfigurationManager:\\n\",\n",
    "    \"    def __init__(\\n\",\n",
    "    \"        self,\\n\",\n",
    "    \"        config_filepath = CONFIG_FILE_PATH,\\n\",\n",
    "    \"        params_filepath = PARAMS_FILE_PATH):\\n\",\n",
    "    \"\\n\",\n",
    "    \"        self.config = read_yaml(config_filepath)\\n\",\n",
    "    \"        self.params = read_yaml(params_filepath)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        create_directories([self.config.artifacts_root])\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\\n\",\n",
    "    \"        config = self.config.model_evaluation\\n\",\n",
    "    \"\\n\",\n",
    "    \"        create_directories([config.root_dir])\\n\",\n",
    "    \"\\n\",\n",
    "    \"        model_evaluation_config = ModelEvaluationConfig(\\n\",\n",
    "    \"            root_dir=config.root_dir,\\n\",\n",
    "    \"            data_path=config.data_path,\\n\",\n",
    "    \"            model_path = config.model_path,\\n\",\n",
    "    \"            tokenizer_path = config.tokenizer_path,\\n\",\n",
    "    \"            metric_file_name = config.metric_file_name\\n\",\n",
    "    \"           \\n\",\n",
    "    \"        )\\n\",\n",
    "    \"\\n\",\n",
    "    \"        return model_evaluation_config\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 9,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\n\",\n",
    "    \"from datasets import load_dataset, load_from_disk, load_metric\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"from tqdm import tqdm\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 10,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class ModelEvaluation:\\n\",\n",
    "    \"    def __init__(self, config: ModelEvaluationConfig):\\n\",\n",
    "    \"        self.config = config\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def generate_batch_sized_chunks(self,list_of_elements, batch_size):\\n\",\n",
    "    \"        \\\"\\\"\\\"split the dataset into smaller batches that we can process simultaneously\\n\",\n",
    "    \"        Yield successive batch-sized chunks from list_of_elements.\\\"\\\"\\\"\\n\",\n",
    "    \"        for i in range(0, len(list_of_elements), batch_size):\\n\",\n",
    "    \"            yield list_of_elements[i : i + batch_size]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def calculate_metric_on_test_ds(self,dataset, metric, model, tokenizer, \\n\",\n",
    "    \"                               batch_size=16, device=\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\", \\n\",\n",
    "    \"                               column_text=\\\"article\\\", \\n\",\n",
    "    \"                               column_summary=\\\"highlights\\\"):\\n\",\n",
    "    \"        article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))\\n\",\n",
    "    \"        target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))\\n\",\n",
    "    \"\\n\",\n",
    "    \"        for article_batch, target_batch in tqdm(\\n\",\n",
    "    \"            zip(article_batches, target_batches), total=len(article_batches)):\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \\n\",\n",
    "    \"                            padding=\\\"max_length\\\", return_tensors=\\\"pt\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            summaries = model.generate(input_ids=inputs[\\\"input_ids\\\"].to(device),\\n\",\n",
    "    \"                            attention_mask=inputs[\\\"attention_mask\\\"].to(device), \\n\",\n",
    "    \"                            length_penalty=0.8, num_beams=8, max_length=128)\\n\",\n",
    "    \"            ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Finally, we decode the generated texts, \\n\",\n",
    "    \"            # replace the  token, and add the decoded texts with the references to the metric.\\n\",\n",
    "    \"            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \\n\",\n",
    "    \"                                    clean_up_tokenization_spaces=True) \\n\",\n",
    "    \"                for s in summaries]      \\n\",\n",
    "    \"            \\n\",\n",
    "    \"            decoded_summaries = [d.replace(\\\"\\\", \\\" \\\") for d in decoded_summaries]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            \\n\",\n",
    "    \"            metric.add_batch(predictions=decoded_summaries, references=target_batch)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        #  Finally compute and return the ROUGE scores.\\n\",\n",
    "    \"        score = metric.compute()\\n\",\n",
    "    \"        return score\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def evaluate(self):\\n\",\n",
    "    \"        device = \\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\"\\n\",\n",
    "    \"        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\\n\",\n",
    "    \"        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\\n\",\n",
    "    \"       \\n\",\n",
    "    \"        #loading data \\n\",\n",
    "    \"        dataset_samsum_pt = load_from_disk(self.config.data_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"        rouge_names = [\\\"rouge1\\\", \\\"rouge2\\\", \\\"rougeL\\\", \\\"rougeLsum\\\"]\\n\",\n",
    "    \"  \\n\",\n",
    "    \"        rouge_metric = load_metric('rouge')\\n\",\n",
    "    \"\\n\",\n",
    "    \"        score = self.calculate_metric_on_test_ds(\\n\",\n",
    "    \"        dataset_samsum_pt['test'][0:10], rouge_metric, model_pegasus, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"\\n\",\n",
    "    \"        rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\\n\",\n",
    "    \"\\n\",\n",
    "    \"        df = pd.DataFrame(rouge_dict, index = ['pegasus'] )\\n\",\n",
    "    \"        df.to_csv(self.config.metric_file_name, index=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        \\n\",\n",
    "    \"\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 11,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"[2023-05-18 20:14:03,142: INFO: common: yaml file: config\\\\config.yaml loaded successfully]\\n\",\n",
    "      \"[2023-05-18 20:14:03,151: INFO: common: yaml file: params.yaml loaded successfully]\\n\",\n",
    "      \"[2023-05-18 20:14:03,153: INFO: common: created directory at: artifacts]\\n\",\n",
    "      \"[2023-05-18 20:14:03,155: INFO: common: created directory at: artifacts/model_evaluation]\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bokti\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_25280\\\\2973449339.py:59: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\\n\",\n",
    "      \"  rouge_metric = load_metric('rouge')\\n\",\n",
    "      \"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:54<00:00, 46.91s/it]\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"[2023-05-18 20:18:18,394: INFO: rouge_scorer: Using default tokenizer.]\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"try:\\n\",\n",
    "    \"    config = ConfigurationManager()\\n\",\n",
    "    \"    model_evaluation_config = config.get_model_evaluation_config()\\n\",\n",
    "    \"    model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\\n\",\n",
    "    \"    model_evaluation_config.evaluate()\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    raise e\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"textS\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.16\"\n",
    "  },\n",
    "  \"orig_nbformat\": 4\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}\n"
   ],
   "id": "df1ff7709c734028"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
